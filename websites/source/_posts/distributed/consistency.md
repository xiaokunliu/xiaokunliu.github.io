---
title: 分布式一致性
category: consistency
date: 2020-06-24 08:09:56
tags: consistency
---



<!-- more -->

### 分布式一致性

在讲述分布式一致性的时候,我们需要明确以下两件事情,其一是分布式事务与一致性的区分,事务的隔离主要是为了避免由于并发执行事务而产生的竞争条件,而分布式一致性主要是为了在遇到延迟和错误时协调副本的状态.两者的关注点并不相同;其二是分布式一致性与分布式共识问题,分布式一致性可以理解为“所有的服务节点看到的数据都是相同的状态”,而分布式共识问题可以理解为“所有的服务节点最终作出相同的决定值/策略”.但是分布式一致性又和上述的事务与共识问题存在着联系,也就是说当我们完成一个分布式事务抑或是服务节点达成共识之后就需要考虑一致性问题.因此对于一致性的讨论离不开共识问题的解决(事务也可以算共识问题的一个子域),因此文章会结合共识问题与一致性展开讨论.



#### 分布式一致性简述

- 复制延迟问题

在一个存储的主从服务架构中,如果此时向主节点发起写请求操作成功之后,在主节点通过binlog进行从服务节点数据同步过程,这个时候主服务节点与从服务节点上看到的数据是不一致的,因为写请求的操作是在不同的时间点到达的,因此不论采取的存储服务是使用哪种架构方案(单leader复制架构/多leader复制架构/无leader复制架构)都会产生短暂时间内的数据的一致性.

- 一致性保证

大部分副本存储层系统都采取最终一致性,即在主节点进行数据同步过程中,期间如果有读取请求落地到从服务节点中,那么这个时候会将读取请求线程挂起并等待主从数据同步完成,当同步完成之后将当前的读取请求线程唤醒以保证当前读取请求服务是读取到最新的一份写入数据.换言之,就是当前的网络发生故障但是最终是可以修复的情况下,存储层服务节点会出现短暂的数据不一致,对于最终一致性有一个更好的名称,即收敛(convergence),因为我们期望存储层的服务节点最终收敛到相同的数据值.

#### 线性一致性

##### 问题描述

在上述的一个数据存储层架构中,主服务节点进行数据同步的时候会产生复制延迟导致从服务节点出现短暂的数据不一致性.那是否存在这样的解决方案,即如果数据库能够给对于读取请求服务的客户端程序而言只有一个数据副本的错觉,那么这个时候对于读取数据的客户端就能够实时读取到一份最新的数据而不需要关心复制延迟的问题?

##### 解决方案的核心思想

要实现上述的需求,在分布式系统中有一个解决方案就是通过线性一致性(也可称为原子一致性/强一致性/直接一致性/外部一致性),其基本思想就是使系统看起来好像只有一个副本,并且对它的所有操作都具备原子性,有了这个保证,即使实际上有很多的数据副本应用程序也不需要再担心复制延迟的问题.

##### 线性一致性分析

> 非一致性的系统模型

- 由于网络延迟问题产生数据不一致

![](/Users/keithl/docker/dev/data/xiaokunliu.github.io/websites/zimages/arch/consistency/consistency.jpg)



- 数据库强一致性分析

![](/Users/keithl/docker/dev/data/xiaokunliu.github.io/websites/zimages/arch/consistency/isoluation_consistency.jpg)



![](/Users/keithl/docker/dev/data/xiaokunliu.github.io/websites/zimages/arch/consistency/isoluation_consistency2.jpg)



![](/Users/keithl/docker/dev/data/xiaokunliu.github.io/websites/zimages/arch/consistency/isoluation_consistency3.jpg)



> 串行化与强一致性区分

- 串行化



- 强一致性





#### 顺序一致性

分布事件的执行顺序一致性问题





#### 最终一致性



#### 服务状态

##### 有状态服务





##### 无状态服务



